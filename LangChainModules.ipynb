{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lharikumar/langchain_intro/blob/main/LangChainModules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the required installs "
      ],
      "metadata": {
        "id": "kuS4ciWRIti6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai python-dotenv langchain tiktoken faiss-cpu datasets wikipedia google-search-results"
      ],
      "metadata": {
        "id": "U1GhYSdvHYZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the key"
      ],
      "metadata": {
        "id": "SZ1zRn1RJ44y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dotenv"
      ],
      "metadata": {
        "id": "uVanRbFGKEsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv('/content/variables/.env')\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "FFA7KbmVKH07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai_api_key = \"Your_api_key\""
      ],
      "metadata": {
        "id": "7i45x5SNKZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "### The different types of AI models that are supported in LangChain\n",
        "\n",
        "*   **LLMs** - These models take a text string as input, and return a text string as output.\n",
        "*   **Chat Models** - These models take a list of Chat Messages as input, and return a Chat Message.\n",
        "*   **Text Embeddings** - These models take text as input and return a list of floats.\n",
        "\n"
      ],
      "metadata": {
        "id": "gM6IsYl4HGDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs"
      ],
      "metadata": {
        "id": "mR7uIPcBOay-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain"
      ],
      "metadata": {
        "id": "fwTxUWIXIrfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
        "llm(\"Tell me a story about animals in 50 words or less\")"
      ],
      "metadata": {
        "id": "JoqSmvT4Ib0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models"
      ],
      "metadata": {
        "id": "jGT5-E6BOcxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,                       # Provide context to the AI \n",
        "    HumanMessage,                        # User query\n",
        "    AIMessage                            # Response from AI Model\n",
        ")"
      ],
      "metadata": {
        "id": "xNnNQVQVPWeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "chat(\n",
        "       [\n",
        "        SystemMessage(content=\"You're an AI bot that can recommend Italian food\"),\n",
        "        HumanMessage(content=\"I am going to Italy next week and want to try out some Italian food. What should I eat\")\n",
        "       ] \n",
        "    )\n"
      ],
      "metadata": {
        "id": "JTH7xEVmPZSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Embeddings"
      ],
      "metadata": {
        "id": "KzyajCiySJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Change the text into a embedding (sequence of numbers that hold the semantic meaning).\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Note that there are different embedding methods for documents (to be searched over) vs queries (the search query itself).\n",
        "query = \"This is a sample query\"\n",
        "query_embedding = embeddings.embed_query(query) ## works over single document\n"
      ],
      "metadata": {
        "id": "GPggxZuYSYp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(query_embedding)"
      ],
      "metadata": {
        "id": "IYzPGHyfT0mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding[0]"
      ],
      "metadata": {
        "id": "yB_1fyUaWCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts\n",
        "\n",
        "\n",
        "*   **LLM Prompt Templates** - Reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
        "*   **Chat Templates** - Chat Models takes a list of chat messages as input - this list commonly referred to as a prompt. These chat messages differ from raw string (which you would pass into a LLM model) in that every message is associated with a role.\n",
        "*   **Example Selectors** - If you have a large number of examples, you may need to select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so.\n",
        "* **Output Parsers** - Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Py2sCGwtXRB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "Bfc3JzJ3XS7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "template = \"\"\"\n",
        "I am travelling to France next week. What are the best museums to visit at {city} ?\n",
        "\n",
        "Respond in 50 words or less.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['city'],\n",
        "    template=template,\n",
        "    validate_template=True,\n",
        ")\n",
        "prompt.format(city='Paris')\n",
        "\n",
        "print(f\"Prompt for the  llm: {prompt.format(city='Paris')}\")"
      ],
      "metadata": {
        "id": "nM6OjhtxXwin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt.format(city='Paris'))"
      ],
      "metadata": {
        "id": "h-ehLm83a3eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Prompt Templates"
      ],
      "metadata": {
        "id": "FK6OFep3dBV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "# from langchain.schema import AIMessage, HumanMessage,  SystemMessage\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt=SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt=HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "ai_template=\"I love Programming : J'adore programmer\"\n",
        "ai_message_prompt=AIMessagePromptTemplate.from_template(ai_template)\n",
        "\n",
        "chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt, ai_message_prompt])\n",
        "chat_prompt_formatted=chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love to eat Pizzas\").to_messages()"
      ],
      "metadata": {
        "id": "aToxl1lsdnVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "chat(chat_prompt_formatted)"
      ],
      "metadata": {
        "id": "To4vOLLjfsU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example selectors"
      ],
      "metadata": {
        "id": "6xdkZSXzgMSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "XZTZuS-OjsIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LengthBased Example Selector -This ExampleSelector selects which examples to use based on length. \n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector"
      ],
      "metadata": {
        "id": "USqMxpGljghP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are a lot of examples \n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sharp\", \"output\": \"blunt\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]"
      ],
      "metadata": {
        "id": "8GiSSrZMjyL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "example_selector = LengthBasedExampleSelector(    \n",
        "    examples=examples,              # Available examples\n",
        "    example_prompt=example_prompt,  # Prompt Template\n",
        "    max_length=25,                  # Length of the string\n",
        ")\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {input_word}\\nOutput:\", \n",
        "    input_variables=[\"input_word\"],\n",
        ")"
      ],
      "metadata": {
        "id": "CwSke-3Nj1k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An example with small input, so it selects all examples.\n",
        "print(dynamic_prompt.format(input_word=\"big\"))"
      ],
      "metadata": {
        "id": "LOMscrU_lFT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An example with long input, so it selects only one example.\n",
        "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
        "print(dynamic_prompt.format(input_word=long_string))"
      ],
      "metadata": {
        "id": "8xhNta0GmCdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Similarity Example Selector\n"
      ],
      "metadata": {
        "id": "xP7gYfS9o-w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The **SemanticSimilarityExampleSelector** selects examples based on which examples are most similar to the inputs. \n",
        "## It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n",
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "Uhl5Q5ehpRRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are a lot of examples of a task of creating antonyms.\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sharp\", \"output\": \"blunt\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")"
      ],
      "metadata": {
        "id": "PkejY4x-pGj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=OpenAIEmbeddings()\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(   \n",
        "    examples=examples,\n",
        "    embeddings=embeddings,\n",
        "    vectorstore_cls=FAISS,\n",
        "    k=2\n",
        ")\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {input_word}\\nOutput:\", \n",
        "    input_variables=[\"input_word\"],\n",
        ")"
      ],
      "metadata": {
        "id": "xWh8FuIisTTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(input_word=\"enthusiastic\"))"
      ],
      "metadata": {
        "id": "cwF__HUxtF5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Other Example Selectors\n",
        "\n",
        "*   The **MaxMarginalRelevanceExampleSelector** selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
        "*   The **NGramOverlapExampleSelector** selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n"
      ],
      "metadata": {
        "id": "lkqGEI1FmNH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Custom Example Selector"
      ],
      "metadata": {
        "id": "v2u4M84ymMDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CustomExampleSelector(BaseExampleSelector):    \n",
        "    def __init__(self, examples: List[Dict[str, str]]):\n",
        "        self.examples = examples\n",
        "    \n",
        "    def add_example(self, example: Dict[str, str]) -> None:\n",
        "        \"\"\"Add new example to store for a key.\"\"\"\n",
        "        self.examples.append(example)\n",
        "\n",
        "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
        "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
        "        return np.random.choice(self.examples, size=2, replace=False)\n"
      ],
      "metadata": {
        "id": "QmxlmAMxnAFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"foo\": \"1\"},\n",
        "    {\"foo\": \"2\"},\n",
        "    {\"foo\": \"3\"},\n",
        "    {\"foo\": \"11\"},\n",
        "    {\"foo\": \"12\"},\n",
        "    {\"foo\": \"13\"},    \n",
        "    {\"foo\": \"111\"},\n",
        "    {\"foo\": \"112\"},\n",
        "    {\"foo\": \"113\"}\n",
        "]\n",
        "\n",
        "# Initialize example selector.\n",
        "example_selector = CustomExampleSelector(examples)\n",
        "\n",
        "# Select examples\n",
        "selected_values=example_selector.select_examples({\"foo\": \"foo\"})\n",
        "print(f\"First selection: {selected_values}\")\n",
        "\n",
        "# Add new example to the set of examples\n",
        "example_selector.add_example({\"foo\": \"44444\"})\n",
        "print(f\"Updated examples: {example_selector.examples}\")\n",
        "\n",
        "# # Select examples\n",
        "# example_selector.select_examples({\"foo\": \"foo\"})\n",
        "print(f\"Second selection: {selected_values}\")"
      ],
      "metadata": {
        "id": "DHNXmGaRnMal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers\n",
        "\n",
        "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "\n",
        "*   CommaSeparatedListOutputParser - output will be a comma separated list\n",
        "*   OutputFixingParser - Parse the LLM response. In case of errors, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it\n",
        "*   PydanticOutputParser - allows users to specify an arbitrary JSON schema and query LLMs for JSON outputs that conform to that schema.\n",
        "*   RetryOutputParser - when the output is not just in the incorrect format, but is partially complete,  the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.\n",
        "*   Structured Output Parser - Define a response schema and convert them to instructions to pass to the prompt"
      ],
      "metadata": {
        "id": "bOtxS8rvvWWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Structed Output Parser\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "j8vG9QySvy5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_schemas = [\n",
        "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
        "    ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\")\n",
        "]\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "GBq9Jc9Jv7xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "DUl3qff4wFeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "_input = prompt.format_prompt(question=\"what's the capital of france\")\n",
        "output = llm(_input.to_string())\n",
        "output_parser.parse(output)"
      ],
      "metadata": {
        "id": "IvjG-B2VwP-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory\n",
        "In chats, it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of “Memory” exists to do exactly that.\n",
        "Memory involves keeping a concept of state around throughout a user’s interactions with an language model\n",
        "\n",
        "Types of memory - https://python.langchain.com/en/latest/modules/memory/how_to_guides.html"
      ],
      "metadata": {
        "id": "itW5X1K-wPnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexes \n",
        "\n",
        "Indexes refer to ways to structure documents so that LLMs can best interact with them. "
      ],
      "metadata": {
        "id": "SFT8PhZ-zLC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Loaders\n",
        "Import data from other sources. For the complete list of loaders refer to https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
      ],
      "metadata": {
        "id": "a47kbM7dzPVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
        "dataset_name=\"imdb\"\n",
        "page_content_column=\"text\"\n",
        "\n",
        "loader=HuggingFaceDatasetLoader(dataset_name,page_content_column)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "k67C1pPn1sXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[10]"
      ],
      "metadata": {
        "id": "vYBlXl0J2ORY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitters\n",
        "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. For the complete list of text splitters supported refer to https://python.langchain.com/en/latest/modules/indexes/text_splitters.html"
      ],
      "metadata": {
        "id": "X9WujWb_3BEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
        "\n",
        "How the text is split: by list of characters\n",
        "\n",
        "How the chunk size is measured: by length function passed in (defaults to number of characters)"
      ],
      "metadata": {
        "id": "CperEQHD0L2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "with open('/content/data/samplefile.txt') as f:\n",
        "  file_data = f.read();\n",
        "\n",
        "print(f\"File Contents first 100 characters: {file_data[:100]}\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=25,\n",
        "                                               chunk_overlap=5)\n",
        "split_text=text_splitter.create_documents([file_data])\n",
        "print(f\"First chunk:  {split_text[0]}\")\n",
        "print(f\"Second chunk: {split_text[1]}\")\n"
      ],
      "metadata": {
        "id": "L46N9rJj0ZWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorStores\n",
        "The most common type of index is one that creates numerical embeddings (with an Embedding Model) for each document. A vectorstore stores Documents and associated embeddings, and provides fast ways to look up relevant Documents by embeddings."
      ],
      "metadata": {
        "id": "GpAe-jZl88B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievers\n",
        "The retriever interface is a generic interface that makes it easy to combine documents with language models. This interface exposes a get_relevant_documents method which takes in a query (a string) and returns a list of documents.\n",
        "Complete list of retrievers is available at https://python.langchain.com/en/latest/modules/indexes/retrievers.html"
      ],
      "metadata": {
        "id": "0fhJE-Kj89D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia Retrievers \n",
        "To retrieve wiki pages from wikipedia.org into the Document format that is used downstream."
      ],
      "metadata": {
        "id": "8GdwHV2v9RvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import WikipediaRetriever\n",
        "retriever = WikipediaRetriever()\n",
        "docs=retriever.get_relevant_documents(query=(\"Midjourney\"))\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "axDf6CCd95oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        " Chains allow us to combine multiple components together to create a single, coherent application.\n",
        " A chain is just an end-to-end wrapper around multiple individual components.\n",
        "\n",
        "\n",
        "*   LLM Chain - A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
        "*   Index related chain - This category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents.\n",
        "\n",
        "For the complete list of chains refer to https://python.langchain.com/en/latest/modules/chains/how_to_guides.html"
      ],
      "metadata": {
        "id": "IsUKmZxh-2G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading from LangChainHub\n",
        "from langchain.chains import load_chain\n",
        "\n",
        "chain = load_chain(\"lc://chains/llm-math/chain.json\")\n",
        "chain.run(\"whats 2 raised to .12\")"
      ],
      "metadata": {
        "id": "OnrYYkQqroRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
        "\n",
        "At the moment, there are two main types of agents:\n",
        "\n",
        "“Action Agents”: these agents decide an action to take and take that action one step at a time\n",
        "\n",
        "“Plan-and-Execute Agents”: these agents first decide a plan of actions to take, and then execute those actions one at a time.\n",
        "\n",
        "*   **Agents** use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning a response to the user.\n",
        "*  **Tools:** these are the actions an agent can take. What tools you give an agent highly depend on what you want the agent to do\n",
        "*  **Toolkits** an agent applied to a particular use case.\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html - List of agents available\n"
      ],
      "metadata": {
        "id": "fPibqujztiAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks.\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import SerpAPIWrapper\n",
        "from langchain.agents.tools import Tool\n",
        "from langchain import LLMMathChain\n",
        "\n",
        "serpapi_api_key=os.getenv(\"SERPAPI_API_KEY\")\n",
        "search = SerpAPIWrapper()\n",
        "llm = OpenAI(temperature=0)\n",
        "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math_chain.run,\n",
        "        description=\"useful for when you need to answer questions about math\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "model = ChatOpenAI(temperature=0)\n",
        "planner = load_chat_planner(model)\n",
        "executor = load_agent_executor(model, tools, verbose=True)\n",
        "agent = PlanAndExecute(planner=planner, executer=executor, verbose=True)\n",
        "\n",
        "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
      ],
      "metadata": {
        "id": "8wyaHEUlthmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Conversation Agent\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "search = SerpAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Current Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events or the current state of the world. the input to this should be a single search term.\"\n",
        "    ),\n",
        "]\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "llm=ChatOpenAI(temperature=0)\n",
        "agent_chain = initialize_agent(tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n",
        "agent_chain.run(input=\"Hi, i am Jane\")"
      ],
      "metadata": {
        "id": "XbeoqvZv2CGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"who am i?\")"
      ],
      "metadata": {
        "id": "175_cVwU2RhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks\n",
        "LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks."
      ],
      "metadata": {
        "id": "JybdhIR4x4O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## StdoutHandler\n",
        "## StdOutCallbackHandler, which simply logs all events to stdout.\n",
        "## when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in.\n",
        "\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "handler = StdOutCallbackHandler()\n",
        "llm = OpenAI()\n",
        "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
        "\n",
        "# First, let's explicitly set the StdOutCallbackHandler in `callbacks`\n",
        "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
        "chain.run(number=2)\n",
        "\n",
        "# Then, let's use the `verbose` flag to achieve the same result\n",
        "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
        "chain.run(number=2)\n",
        "\n",
        "# Finally, let's use the request `callbacks` to achieve the same result\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain.run(number=3, callbacks=[handler])"
      ],
      "metadata": {
        "id": "6-LJsydWyBTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}